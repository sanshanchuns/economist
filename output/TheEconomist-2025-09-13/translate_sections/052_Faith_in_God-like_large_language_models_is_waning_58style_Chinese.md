<span style="color:#E3120B; font-size:14.9pt; font-weight:bold;">商业</span> <span style="color:#000000; font-size:14.9pt; font-weight:bold;">| 大模型天花板？</span>
<span style="color:#000000; font-size:21.0pt; font-weight:bold;">“神一样”的大语言模型，信仰在降温</span>
<span style="color:#808080; font-size:14.9pt; font-weight:bold; font-style:italic;">这或许是苹果等“慢热派”的好消息</span>
<span style="color:#808080; font-size:6.2pt;">2025年9月11日</span>

<div style="padding:8px 12px; color:#666; font-size:9.0pt; font-style:italic; margin:12px 0;">摘要：前沿大模型的迭代，越来越像“手机小升级”。企业用户转而青睐“小而专”的SLM（小语言模型）：更省钱、更好落地、能上端侧，还能当Agent的“工作引擎”。大模型不会消失，但“异构共存”才是新常态。</div>

![](../images/052_Faith_in_God-like_large_language_models_is_waning/p0212_img01.jpeg)

【一｜神话褪色：从“iPhone时刻”到“常规换代”】

ChatGPT刚出时堪比2007年的iPhone，如今像iPhone 17那样“升级但不惊艳”。GPT-5引发的是“集体耸肩”。边界进步放缓，意味着“大而全”的神话在降温。

【二｜企业转向“小而专”】

- SLM更省：参数量从几十亿到2.5亿量级，训练/推理成本骤降；
- 更好用：能在自有机房或云上跑，便于私有化；
- 更贴身：HR机器人不需要懂高能物理；行业微调更稳更准。

【三｜端侧与Agent的“黄金搭档”】

SLM更适合手机、车端、机器人等“算力/功耗敏感”场景。英伟达研究直言：Agent的未来更偏“小模型拼装”，而非“一模通吃”。7B参数模型的运行成本可低一个数量级以上。

【四｜性能追平的路径】

“小教大”变“大教小”：小模型不必自己全网爬，更多由大模型蒸馏教学。9B的Nemotron Nano在多项基准上超越体量大40倍的Llama版本——“小追大”已成常态。

【五｜经济账：别拿777飞洛杉矶】

企业从“砸钱求用”转向“算账求效”。简单工单用SLM，复杂任务再上LLM。IBM用2.5亿参数的小模型做Docling把PDF变结构化数据，若用LLM并不划算。小模型还能跑CPU，不必人人等GPU的“法拉利”。

【六｜异构共存：各取所长】

- 消费侧：通用LLM仍重要（如ChatGPT）；
- 企业侧：行业SLM需求增速翻倍（基数低但快）；
- 研发侧：继续推进LLM前沿，以便更好“教”SLM；
- 模型内生异构：像GPT-5内部已按任务难度调用不同“体型”的子模型。

【七｜对“慢热派”的意义】

苹果“端上做小活、云上做重活”的路线被看好。虽错过开局红利，但凭生态与分发，“下一班船”依然能上。长期看，押注“好用、够用、能落地”的SLM，是稳健解。

大模型不会退出舞台，但“单栈神力”让位于“多模协同”。把钱花在刀刃上：该用大就用大，该小就小——这才是AI落地的真实世界逻辑。
